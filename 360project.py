# -*- coding: utf-8 -*-
"""360project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N24IF6NBCEQoNvCML8CWepZiRAe-Lues
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms.functional as TF
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from torch.cuda.amp import GradScaler, autocast
import pandas as pd
import torch.utils.data as data
from torchvision import transforms
from PIL import Image
from torch.utils.data import Dataset, DataLoader, Subset
import pandas as pd
import os
import random

import Train
import Model

#from google.colab import drive
#drive.mount('/content/drive')

"""Unzip the test data"""

# !mkdir -p /content/drive/MyDrive/APS360/Project/Test
# !unzip /content/drive/MyDrive/APS360/Project/Test.zip -d /content/drive/MyDrive/APS360/Project/Test
# !ls /content/drive/MyDrive/APS360/Project/Test


# Define the ColorFilterLayer

def total_train():
    """Setting up Training for the Model"""

    # Load data and prepare the model
    train_loader, val_loader, class_names = Train.load_data(batch_size=32)
    num_classes = len(class_names)

    # Initialize model, loss function, and optimizer
    model = Model.TrafficSignGoogLeNet(num_classes=num_classes).to(Train.device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    print(next(model.parameters()).device)  # Should show 'cuda:0' if the model is on GPU

    # Train the model and plot training curves
    Train.train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=30)

    # Visualize predictions on validation set
    Train.visualize_predictions(model, val_loader, class_names)

    Train.visualize_misclassified(model, val_loader, class_names, num_images=9)



if __name__ == '__main__':

    total_train()

    # Load the dataset with label names
    test_dataset = Train.GTSRBTestDataset(
    csv_file=Train.save_dir+'\\Test.csv',
    root_dir=Train.save_dir,
    )

    # Randomly sample a subset of 500 images
    num_samples = 1290
    indices = list(range(len(test_dataset)))
    random_sampled_indices = random.sample(indices, num_samples)
    test_subset = Subset(test_dataset, random_sampled_indices)

    # Create DataLoader for the subset
    test_loader = DataLoader(dataset=test_subset, batch_size=32, shuffle=True)
    print(f'Number of test samples: {len(test_subset)}')
    '''
    # Verify the data
    for i, (images, labels) in enumerate(test_loader):
        print(f"Batch {i + 1}:")
        print(f"Images batch shape: {images.shape}")
        print(f"Labels batch shape: {labels.shape}")
        print(f"Labels: {labels}\n")

        # Stop after the first few batches for debugging purposes
        if i == 0:  # You can adjust the number of batches to inspect
            break

    '''
    # Instantiate the model
    train_loader, val_loader, class_names = Train.load_data(batch_size=32)
    num_classes = len(class_names)

    # Load the saved weights
    model_path = Train.save_dir+'\\model_epoch_30.pth'
    checkpoint = torch.load(model_path, map_location=torch.device('cpu'),weights_only=True)

    # Ensure your model architecture matches the one used during training
    model = Model.TrafficSignGoogLeNet(num_classes=num_classes).to(Train.device)

    # Load the state_dict for the model only
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # Switch the model to evaluation mode
    model.eval()
    
    for images, labels in test_loader:
        outputs = model(images.to(Train.device))
        _, predictions = torch.max(outputs, 1)
        print(f"Labels: {labels}")
        print(f"Predictions: {predictions}")
        break
    

    # Test the model on the test dataset
    test_accuracy, top_1_error_rate = Train.test_model(model, test_loader)

    # Print test results
    print(f"Test Accuracy: {test_accuracy:.2f}%")
    print(f"Top-1 Error Rate: {top_1_error_rate:.2f}%")